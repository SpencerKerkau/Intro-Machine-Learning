{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2A: Implement the two layer neural network in ‘simple neural network.ipynb’ using keras. Modify the loss function so it is designed for binary classification\n",
    "\n",
    "Response/Comments for problem 2a\n",
    "\n",
    "In the code block below, I changed the loss function from mean squared error to cross entropy. The cross entropy loss function was already precoded so I commented out the original mean squared error and uncommented the cross entropy function.\n",
    "\n",
    "When using the mean squared error loss, after the final epoch, the validation loss was 0.0661~ and the validation accuracy was 1.0. When comparing to the cross entropy loss, we find that the validation loss is 0.2761~ and the validation accuracy is 1.0. A slightly different validation loss is to be expected as MSE is not typically suited for classification problems and more tailored for handling regression problems due to the nature of the ouput (i.e. Classification outputs being 1 or 0 versus a scalar value for regression)\n",
    "\n",
    "Mean Squared Error Loss Table\n",
    "1. Epoch:     0 ; Validation loss:  0.2674 ; Validation accuracy:   0.625\n",
    "2. Epoch:   100 ; Validation loss:  0.1989 ; Validation accuracy:  0.7375\n",
    "3. Epoch:   200 ; Validation loss:  0.1629 ; Validation accuracy:  0.8125\n",
    "4. Epoch:   300 ; Validation loss:  0.1361 ; Validation accuracy:    0.85\n",
    "5. Epoch:   400 ; Validation loss:  0.1161 ; Validation accuracy:  0.8625\n",
    "6. Epoch:   500 ; Validation loss:   0.101 ; Validation accuracy:  0.9125\n",
    "7. Epoch:   600 ; Validation loss:  0.0893 ; Validation accuracy:  0.9375\n",
    "8. Epoch:   700 ; Validation loss:  0.0799 ; Validation accuracy:    0.95\n",
    "9. Epoch:   800 ; Validation loss:  0.0723 ; Validation accuracy:  0.9875\n",
    "10. Epoch:   900 ; Validation loss:  0.0661 ; Validation accuracy:     1.0\n",
    "\n",
    "Cross Entropy Loss Table\n",
    "1. Epoch:     0 ; Validation loss:  0.8878 ; Validation accuracy:  0.6125\n",
    "2. Epoch:   100 ; Validation loss:  0.6305 ; Validation accuracy:  0.6875\n",
    "3. Epoch:   200 ; Validation loss:  0.5569 ; Validation accuracy:   0.825\n",
    "4. Epoch:   300 ; Validation loss:  0.4888 ; Validation accuracy:  0.8875\n",
    "5. Epoch:   400 ; Validation loss:  0.4306 ; Validation accuracy:  0.9125\n",
    "6. Epoch:   500 ; Validation loss:  0.3836 ; Validation accuracy:   0.925\n",
    "7. Epoch:   600 ; Validation loss:  0.3462 ; Validation accuracy:  0.9375\n",
    "8. Epoch:   700 ; Validation loss:  0.3171 ; Validation accuracy:  0.9875\n",
    "9. Epoch:   800 ; Validation loss:  0.2944 ; Validation accuracy:  0.9875\n",
    "10. Epoch:   900 ; Validation loss:  0.2761 ; Validation accuracy:     1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     0 ; Validation loss:  0.7055 ; Validation accuracy:  0.6375\n",
      "Epoch:   100 ; Validation loss:  0.6059 ; Validation accuracy:   0.725\n",
      "Epoch:   200 ; Validation loss:  0.5306 ; Validation accuracy:     0.8\n",
      "Epoch:   300 ; Validation loss:  0.4689 ; Validation accuracy:   0.825\n",
      "Epoch:   400 ; Validation loss:  0.4177 ; Validation accuracy:  0.8875\n",
      "Epoch:   500 ; Validation loss:  0.3763 ; Validation accuracy:   0.925\n",
      "Epoch:   600 ; Validation loss:  0.3432 ; Validation accuracy:  0.9375\n",
      "Epoch:   700 ; Validation loss:  0.3165 ; Validation accuracy:  0.9875\n",
      "Epoch:   800 ; Validation loss:  0.2947 ; Validation accuracy:  0.9875\n",
      "Epoch:   900 ; Validation loss:  0.2764 ; Validation accuracy:     1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# We will be using make_circles from scikit-learn\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "SEED = 2017\n",
    "\n",
    "# We create an inner and outer circle\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05, random_state=2017)\n",
    "outer = y == 0\n",
    "inner = y == 1\n",
    "\n",
    "# plt.title(\"Two Circles\")\n",
    "# plt.plot(X[outer, 0], X[outer, 1], \"ro\")\n",
    "# plt.plot(X[inner, 0], X[inner, 1], \"bo\")\n",
    "# plt.show()\n",
    "\n",
    "#print(X)\n",
    "X = X+1\n",
    "#print(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "n_hidden = 50 # number of hidden units\n",
    "n_epochs = 1000\n",
    "learning_rate = 1 \n",
    "\n",
    "# Initialise weights\n",
    "weights_hidden = np.random.normal(0.0, size=(X_train.shape[1], n_hidden))\n",
    "weights_output = np.random.normal(0.0, size=(n_hidden))\n",
    "\n",
    "hist_loss = []\n",
    "hist_accuracy = []\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    del_w_hidden = np.zeros(weights_hidden.shape)\n",
    "    del_w_output = np.zeros(weights_output.shape)\n",
    "\n",
    "    # Loop through training data in batches of 1\n",
    "    for x_, y_ in zip(X_train, y_train):\n",
    "        # Forward computations\n",
    "        hidden_input = np.dot(x_, weights_hidden)\n",
    "        hidden_output = logistic(hidden_input)\n",
    "        output = logistic(np.dot(hidden_output, weights_output)) #p( y = 1 | x)\n",
    "\n",
    "        # Backward computations\n",
    "\n",
    "        error = y_ - output #we can use this since y is either 0 or 1\n",
    "        output_error = error * output * (1 - output)  #this is the gradient of logistic function\n",
    "        \n",
    "        hidden_error = np.dot(output_error, weights_output) * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        del_w_output += output_error * hidden_output\n",
    "        del_w_hidden += hidden_error * x_[:, None]\n",
    "\n",
    "    # Update weights\n",
    "    weights_hidden += learning_rate * del_w_hidden / X_train.shape[0]\n",
    "    weights_output += learning_rate * del_w_output / X_train.shape[0]\n",
    "\n",
    "    # Print stats (validation loss and accuracy)\n",
    "    if e % 100 == 0:\n",
    "        hidden_output = logistic(np.dot(X_val, weights_hidden))\n",
    "        out = logistic(np.dot(hidden_output, weights_output))\n",
    "\n",
    "        #mean square error \n",
    "        #loss = np.mean((out - y_val) ** 2)\n",
    "\n",
    "        #cross entropy error\n",
    "        loss  = np.mean(-(y_val *np.log(out)) - ((1-y_val)*np.log(1-out)))\n",
    "\n",
    "        # Final prediction is based on a threshold of 0.5\n",
    "        predictions = out > 0.5\n",
    "        accuracy = np.mean(predictions == y_val)\n",
    "        print(\"Epoch: \", '{:>4}'.format(e), \n",
    "            \"; Validation loss: \", '{:>6}'.format(loss.round(4)), \n",
    "            \"; Validation accuracy: \", '{:>6}'.format(accuracy.round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2B: Modify the three layer neural network in ‘wine-classify.ipynb’ so it is solving a multi-class classification problem instead of a regression problem.\n",
    "\n",
    "---\n",
    "\n",
    "# Response/Comments for problem 2b\n",
    "\n",
    "- Epoch 2000/2000\n",
    "- 20/20 - 0s - 2ms/step - accuracy: 0.9953 - loss: 0.0175 - val_accuracy: 0.6562 - val_loss: 4.1651\n",
    "- 10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 523us/step - accuracy: 0.6478 - loss: 4.5418\n",
    "- Test accuracy: 65.62%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.3862 - loss: 1.5650 - val_accuracy: 0.5750 - val_loss: 1.3345\n",
      "Epoch 2/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.5668 - loss: 1.2154 - val_accuracy: 0.5688 - val_loss: 1.0883\n",
      "Epoch 3/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.5747 - loss: 1.0650 - val_accuracy: 0.6062 - val_loss: 0.9936\n",
      "Epoch 4/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.5966 - loss: 1.0095 - val_accuracy: 0.6000 - val_loss: 0.9715\n",
      "Epoch 5/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6083 - loss: 0.9835 - val_accuracy: 0.5906 - val_loss: 0.9561\n",
      "Epoch 6/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6091 - loss: 0.9613 - val_accuracy: 0.6031 - val_loss: 0.9450\n",
      "Epoch 7/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6153 - loss: 0.9472 - val_accuracy: 0.6094 - val_loss: 0.9342\n",
      "Epoch 8/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6200 - loss: 0.9328 - val_accuracy: 0.6187 - val_loss: 0.9249\n",
      "Epoch 9/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6271 - loss: 0.9225 - val_accuracy: 0.6062 - val_loss: 0.9300\n",
      "Epoch 10/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6224 - loss: 0.9092 - val_accuracy: 0.6156 - val_loss: 0.9174\n",
      "Epoch 11/20\n",
      "20/20 - 0s - 3ms/step - accuracy: 0.6325 - loss: 0.9012 - val_accuracy: 0.6187 - val_loss: 0.9128\n",
      "Epoch 12/20\n",
      "20/20 - 0s - 2ms/step - accuracy: 0.6286 - loss: 0.8929 - val_accuracy: 0.5938 - val_loss: 0.9165\n",
      "Epoch 13/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6310 - loss: 0.8873 - val_accuracy: 0.6187 - val_loss: 0.9004\n",
      "Epoch 14/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6466 - loss: 0.8758 - val_accuracy: 0.6031 - val_loss: 0.9131\n",
      "Epoch 15/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6357 - loss: 0.8721 - val_accuracy: 0.6187 - val_loss: 0.8978\n",
      "Epoch 16/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6450 - loss: 0.8625 - val_accuracy: 0.6281 - val_loss: 0.8904\n",
      "Epoch 17/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6536 - loss: 0.8529 - val_accuracy: 0.6219 - val_loss: 0.8945\n",
      "Epoch 18/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6489 - loss: 0.8516 - val_accuracy: 0.6250 - val_loss: 0.8892\n",
      "Epoch 19/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6560 - loss: 0.8460 - val_accuracy: 0.6187 - val_loss: 0.8919\n",
      "Epoch 20/20\n",
      "20/20 - 0s - 1ms/step - accuracy: 0.6568 - loss: 0.8411 - val_accuracy: 0.6469 - val_loss: 0.8869\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - accuracy: 0.6291 - loss: 0.9419\n",
      "Test accuracy: 64.69%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input\n",
    "from keras.backend import floatx\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SEED = 2017\n",
    "\n",
    "pathname = '/Users/spencerkerkau/Desktop/Lec7code/Data/winequality-red.csv'\n",
    "data = pd.read_csv(pathname, sep=';')\n",
    "\n",
    "y = data['quality']\n",
    "X = data.drop(['quality'], axis=1)\n",
    "\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y = to_categorical(encoded_Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Use Input(shape) as the first layer\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "# First hidden layer with 100 hidden units\n",
    "model.add(Dense(200, activation='relu')) \n",
    "\n",
    "# Second hidden layer with 50 hidden units\n",
    "model.add(Dense(25, activation = 'relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "# Set optimizer\n",
    "opt = Adam()\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "             EarlyStopping(monitor='val_loss', patience=20, verbose=2),\n",
    "             ModelCheckpoint('checkpoints/multi_layer_best_model.keras', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "            ]\n",
    "\n",
    "n_batch_size = 64\n",
    "n_epochs = 20 #5000\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, batch_size=n_batch_size, epochs=n_epochs, validation_split=0.2, verbose=2, validation_data=(X_test, y_test))\n",
    "\n",
    "best_model = model\n",
    "best_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Evaluate on test set\n",
    "score = best_model.evaluate(X_test.values, y_test, verbose=1)\n",
    "print('Test accuracy: %.2f%%' % (score[1]*100))\n",
    "\n",
    "# Test accuracy: 65.62% \n",
    "# Benchmark accuracy on dataset 62.4%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
