{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd  # type: ignore\n",
    "\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "from imblearn.combine import SMOTETomek # type: ignore\n",
    "from imblearn.under_sampling import TomekLinks # type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler # type: ignore\n",
    "\n",
    "import torch # type: ignore\n",
    "import torch.nn as nn # type: ignore\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim # type: ignore\n",
    "\n",
    "from sklearn.metrics import f1_score # type: ignore\n",
    "\n",
    "import optuna # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model with no data feature scaling or data imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_sm: 96.83 % of the dataset\n",
      "asd_sm: 1.43 % of the dataset\n",
      "kd_sm: 0.64 % of the dataset\n",
      "rhd_sm: 1.1 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "asd_sm = pd.read_csv('asd_sm.csv')\n",
    "kd_sm = pd.read_csv('kd_sm.csv')\n",
    "normal_sm = pd.read_csv('normal_sm.csv')\n",
    "rhd_sm = pd.read_csv('rhd_sm.csv')\n",
    "\n",
    "normal_sm['state'] = 0\n",
    "asd_sm['state'] = 1\n",
    "kd_sm['state'] = 2\n",
    "rhd_sm['state'] = 3\n",
    "\n",
    "combined_df = pd.concat([normal_sm, asd_sm, kd_sm, rhd_sm], axis=0)\n",
    "\n",
    "print('normal_sm:', round(combined_df['state'].value_counts()[0]/len(combined_df) * 100,2), '% of the dataset')\n",
    "print('asd_sm:', round(combined_df['state'].value_counts()[1]/len(combined_df) * 100,2), '% of the dataset')\n",
    "print('kd_sm:', round(combined_df['state'].value_counts()[2]/len(combined_df) * 100,2), '% of the dataset')\n",
    "print('rhd_sm:', round(combined_df['state'].value_counts()[3]/len(combined_df) * 100,2), '% of the dataset')\n",
    "\n",
    "raw_data = combined_df.copy()\n",
    "raw_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model With No Data Scaling or Adjustments -> Training F1 Score: 0.4393785730496816 | Validation F1 Score: 0.36660724503361686\n"
     ]
    }
   ],
   "source": [
    "X = raw_data.drop('state', axis=1)\n",
    "y = raw_data['state']\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(n_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.input_layer(x) \n",
    "\n",
    "X_train = torch.FloatTensor(X_train.to_numpy())\n",
    "y_train = torch.LongTensor(y_train.to_numpy().astype(int))  \n",
    "\n",
    "X_validation = torch.FloatTensor(X_validation.to_numpy())\n",
    "y_validation = torch.LongTensor(y_validation.to_numpy().astype(int))  \n",
    "\n",
    "model = LogisticRegressionModel(n_features=170, n_classes=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch_count = 1000\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_train_prediction = model(X_train)\n",
    "    training_loss = criterion(y_train_prediction, y_train)\n",
    "    training_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "y_train_prediction = torch.argmax(y_train_prediction, dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_validation_prediction = model(X_validation)\n",
    "    y_validation_prediction = torch.argmax(y_validation_prediction, dim=1)\n",
    "    \n",
    "Training_F1 = f1_score(y_train.detach().numpy(), y_train_prediction.detach().numpy(), average='macro')\n",
    "Validation_F1 = f1_score(y_validation.detach().numpy(), y_validation_prediction.detach().numpy(), average='macro')\n",
    "\n",
    "\n",
    "print(f'Model With No Data Scaling or Adjustments -> Training F1 Score: {Training_F1} | Validation F1 Score: {Validation_F1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set F1 Score: 0.38886370524134256\n"
     ]
    }
   ],
   "source": [
    "# Test_Set = pd.read_csv('test_all.csv').copy()\n",
    "\n",
    "# X = Test_Set.drop('state', axis=1)\n",
    "# y = Test_Set['state']\n",
    "\n",
    "# mapping = {\n",
    "#     'normal': 0,\n",
    "#     'asd': 1,\n",
    "#     'kd': 2,\n",
    "#     'rhd': 3\n",
    "# }\n",
    "\n",
    "# y = y.replace(mapping)\n",
    "\n",
    "# X_test = torch.FloatTensor(X.to_numpy())\n",
    "# y_test = torch.LongTensor(y.to_numpy().astype(int)) \n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y_test_prediction = model(X_test)\n",
    "#     y_test_prediction = torch.argmax(y_test_prediction, dim=1)\n",
    "    \n",
    "\n",
    "# Test_F1 = f1_score(y_test.detach().numpy(), y_test_prediction.detach().numpy(), average='macro')\n",
    "\n",
    "# print(f'Test Set F1 Score: {Test_F1}')\n",
    "\n",
    "# y_test_prediction = pd.DataFrame(y_test_prediction, columns=['prediction'])\n",
    "\n",
    "# # Now saving it to a CSV file\n",
    "# y_test_prediction.to_csv('Logistic_Regression_Answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model with data feature scaling and data imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_sm: 25.0 % of the dataset\n",
      "asd_sm: 25.0 % of the dataset\n",
      "kd_sm: 25.0 % of the dataset\n",
      "rhd_sm: 25.0 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "X = raw_data.drop('state', axis=1)\n",
    "y = raw_data['state']\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "def Standard_Scaler (df, col_names):\n",
    "    features = df[col_names]\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    df[col_names] = features\n",
    "    \n",
    "    return df\n",
    "\n",
    "col_names = X.columns\n",
    "\n",
    "X_train = Standard_Scaler(X_train, col_names)\n",
    "X_validation = Standard_Scaler(X_validation, col_names)\n",
    "\n",
    "resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "\n",
    "X_train_resampled, y_train_resampled = resample.fit_resample(X_train, y_train)\n",
    "\n",
    "print('normal_sm:', round(y_train_resampled.value_counts()[0]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('asd_sm:', round(y_train_resampled.value_counts()[1]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('kd_sm:', round(y_train_resampled.value_counts()[2]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('rhd_sm:', round(y_train_resampled.value_counts()[3]/len(y_train_resampled) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model With Data Scaling and Adjustments -> Training F1 Score: 0.8251489176416756 | Validation F1 Score: 0.26127799267646457\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(n_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.input_layer(x) \n",
    "\n",
    "X_train_resampled = torch.FloatTensor(X_train_resampled.to_numpy())\n",
    "y_train_resampled = torch.LongTensor(y_train_resampled.to_numpy().astype(int))\n",
    "\n",
    "X_validation = torch.FloatTensor(X_validation.to_numpy())\n",
    "y_validation = torch.LongTensor(y_validation.to_numpy().astype(int))  \n",
    "\n",
    "model = LogisticRegressionModel(n_features=170, n_classes=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch_count = 1000\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_train_prediction = model(X_train_resampled)\n",
    "    training_loss = criterion(y_train_prediction, y_train_resampled)\n",
    "    training_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "y_train_prediction = torch.argmax(y_train_prediction, dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_validation_prediction = model(X_validation)\n",
    "    y_validation_prediction = torch.argmax(y_validation_prediction, dim=1)\n",
    "    \n",
    "Training_F1 = f1_score(y_train_resampled.detach().numpy(), y_train_prediction.detach().numpy(), average='macro')\n",
    "Validation_F1 = f1_score(y_validation.detach().numpy(), y_validation_prediction.detach().numpy(), average='macro')\n",
    "\n",
    "print(f'Model With Data Scaling and Adjustments -> Training F1 Score: {Training_F1} | Validation F1 Score: {Validation_F1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model With Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_sm: 25.0 % of the dataset\n",
      "asd_sm: 25.0 % of the dataset\n",
      "kd_sm: 25.0 % of the dataset\n",
      "rhd_sm: 25.0 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "X = raw_data.drop('state', axis=1)\n",
    "y = raw_data['state']\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "def Standard_Scaler (df, col_names):\n",
    "    features = df[col_names]\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    df[col_names] = features\n",
    "    \n",
    "    return df\n",
    "\n",
    "col_names = X.columns\n",
    "\n",
    "X_train = Standard_Scaler(X_train, col_names)\n",
    "X_validation = Standard_Scaler(X_validation, col_names)\n",
    "\n",
    "resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "\n",
    "X_train_resampled, y_train_resampled = resample.fit_resample(X_train, y_train)\n",
    "\n",
    "print('normal_sm:', round(y_train_resampled.value_counts()[0]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('asd_sm:', round(y_train_resampled.value_counts()[1]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('kd_sm:', round(y_train_resampled.value_counts()[2]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('rhd_sm:', round(y_train_resampled.value_counts()[3]/len(y_train_resampled) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression With Hyperparameter Tuning Using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-10 21:39:11,185] A new study created in memory with name: no-name-04a00203-229f-404a-853d-4699e9dd2bbe\n",
      "[I 2024-05-10 21:39:22,214] Trial 0 finished with value: 0.28009480425865907 and parameters: {'optimizer': 'Adam', 'lr': 0.0017244961007919966, 'n_epochs': 1112}. Best is trial 0 with value: 0.28009480425865907.\n",
      "[I 2024-05-10 21:40:04,608] Trial 1 finished with value: 0.29792750562941483 and parameters: {'optimizer': 'Adam', 'lr': 0.00648377787571706, 'n_epochs': 4285}. Best is trial 1 with value: 0.29792750562941483.\n",
      "[I 2024-05-10 21:40:17,646] Trial 2 finished with value: 0.2951235166972829 and parameters: {'optimizer': 'Adam', 'lr': 0.021438172263667975, 'n_epochs': 1352}. Best is trial 1 with value: 0.29792750562941483.\n",
      "[I 2024-05-10 21:40:45,599] Trial 3 finished with value: 0.28960071249117464 and parameters: {'optimizer': 'SGD', 'lr': 0.048977897777363576, 'n_epochs': 2952}. Best is trial 1 with value: 0.29792750562941483.\n",
      "[I 2024-05-10 21:41:05,798] Trial 4 finished with value: 0.2862866875473775 and parameters: {'optimizer': 'SGD', 'lr': 0.05143105104105206, 'n_epochs': 2062}. Best is trial 1 with value: 0.29792750562941483.\n",
      "[I 2024-05-10 21:41:42,137] Trial 5 finished with value: 0.29386639387542995 and parameters: {'optimizer': 'Adam', 'lr': 0.001906938714316936, 'n_epochs': 3600}. Best is trial 1 with value: 0.29792750562941483.\n",
      "[I 2024-05-10 21:41:55,572] Trial 6 finished with value: 0.2882948027188679 and parameters: {'optimizer': 'SGD', 'lr': 0.09249403199841173, 'n_epochs': 1344}. Best is trial 1 with value: 0.29792750562941483.\n",
      "[I 2024-05-10 21:42:38,643] Trial 7 finished with value: 0.29904440011145234 and parameters: {'optimizer': 'Adam', 'lr': 0.016035075295773086, 'n_epochs': 4253}. Best is trial 7 with value: 0.29904440011145234.\n",
      "[I 2024-05-10 21:43:43,576] Trial 8 finished with value: 0.29119881274262277 and parameters: {'optimizer': 'Adam', 'lr': 0.0010097545570751708, 'n_epochs': 4965}. Best is trial 7 with value: 0.29904440011145234.\n",
      "[I 2024-05-10 21:44:05,656] Trial 9 finished with value: 0.29735068499855044 and parameters: {'optimizer': 'Adam', 'lr': 0.0244798534955577, 'n_epochs': 1884}. Best is trial 7 with value: 0.29904440011145234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.29904440011145234\n",
      "  Params: \n",
      "    optimizer: Adam\n",
      "    lr: 0.016035075295773086\n",
      "    n_epochs: 4253\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(n_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.input_layer(x) \n",
    "    \n",
    "\n",
    "X_train_resampled_tensor = torch.FloatTensor(X_train_resampled.to_numpy())\n",
    "y_train_resampled_tensor = torch.LongTensor(y_train_resampled.to_numpy().astype(int))  \n",
    "\n",
    "X_validation_tensor = torch.FloatTensor(X_validation.to_numpy())\n",
    "y_validation_tensor = torch.LongTensor(y_validation.to_numpy().astype(int))  \n",
    "\n",
    "def create_objective(X_train, y_train, X_val, y_val):\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "        lr = trial.suggest_float(\"lr\", 1e-3, 1e-1, log=True)\n",
    "        n_epochs = trial.suggest_int(\"n_epochs\", 1000, 5000)\n",
    "\n",
    "        model = LogisticRegressionModel(n_features=170, n_classes=4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_val)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            val_f1 = f1_score(y_val.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n",
    "\n",
    "        return val_f1\n",
    "    \n",
    "    return objective\n",
    "\n",
    "# Now create the objective function using the closure\n",
    "objective = create_objective(X_train_resampled_tensor, y_train_resampled_tensor, X_validation_tensor, y_validation_tensor)\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
