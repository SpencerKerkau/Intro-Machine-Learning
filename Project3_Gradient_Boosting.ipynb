{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Classifier with no data feature scaling or data imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_sm: 96.83 % of the dataset\n",
      "asd_sm: 1.43 % of the dataset\n",
      "kd_sm: 0.64 % of the dataset\n",
      "rhd_sm: 1.1 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "asd_sm = pd.read_csv('asd_sm.csv')\n",
    "kd_sm = pd.read_csv('kd_sm.csv')\n",
    "normal_sm = pd.read_csv('normal_sm.csv')\n",
    "rhd_sm = pd.read_csv('rhd_sm.csv')\n",
    "\n",
    "normal_sm['state'] = 0\n",
    "asd_sm['state'] = 1\n",
    "kd_sm['state'] = 2\n",
    "rhd_sm['state'] = 3\n",
    "\n",
    "combined_df = pd.concat([normal_sm, asd_sm, kd_sm, rhd_sm], axis=0)\n",
    "\n",
    "print('normal_sm:', round(combined_df['state'].value_counts()[0]/len(combined_df) * 100,2), '% of the dataset')\n",
    "print('asd_sm:', round(combined_df['state'].value_counts()[1]/len(combined_df) * 100,2), '% of the dataset')\n",
    "print('kd_sm:', round(combined_df['state'].value_counts()[2]/len(combined_df) * 100,2), '% of the dataset')\n",
    "print('rhd_sm:', round(combined_df['state'].value_counts()[3]/len(combined_df) * 100,2), '% of the dataset')\n",
    "\n",
    "raw_data = combined_df.copy()\n",
    "raw_data.drop_duplicates(inplace=True)\n",
    "\n",
    "X = raw_data.drop('state', axis=1)\n",
    "y = raw_data['state']\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Classifier with no data feature scaling or data imbalance handling\n",
      "--------------------------------------------------\n",
      "Gradient Boosted Classifier: Training F1 Score: 0.41018967518361493 | Validation F1 Score: 0.3734732918011756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf_GBC.fit(X_train, y_train)\n",
    "training_output = clf_GBC.predict(X_train)\n",
    "training_f1 = f1_score(y_train, training_output, average='macro')\n",
    "\n",
    "Validation_Output = clf_GBC.predict(X_validation)\n",
    "Validation_f1 = f1_score(y_validation, Validation_Output, average='macro')\n",
    "\n",
    "print('Gradient Boosted Classifier with no data feature scaling or data imbalance handling')\n",
    "print('-' * 50)\n",
    "print(f'Gradient Boosted Classifier: Training F1 Score: {training_f1} | Validation F1 Score: {Validation_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set F1 Score: 0.24600159052752496\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option('future.no_silent_downcasting', True)\n",
    "# Test_Set = pd.read_csv('test_all.csv').copy()\n",
    "\n",
    "# X = Test_Set.drop('state', axis=1)\n",
    "# y = Test_Set['state']\n",
    "\n",
    "# mapping = {\n",
    "#     'normal': 0,\n",
    "#     'asd': 1,\n",
    "#     'kd': 2,\n",
    "#     'rhd': 3\n",
    "# }\n",
    "\n",
    "# y = y.replace(mapping)\n",
    "# y.fillna(0, inplace=True)\n",
    "# y = y.astype(int)\n",
    "\n",
    "# def Standard_Scaler (df, col_names):\n",
    "#     features = df[col_names]\n",
    "#     scaler = StandardScaler().fit(features.values)\n",
    "#     features = scaler.transform(features.values)\n",
    "#     df[col_names] = features\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# col_names = X.columns\n",
    "\n",
    "# X_test = Standard_Scaler(X, col_names)\n",
    "\n",
    "# # Assuming clf_GBC is already trained and available\n",
    "# test_output = clf_GBC.predict(X_test)\n",
    "# test_f1 = f1_score(y, test_output, average='macro')\n",
    "\n",
    "# print(f'Test Set F1 Score: {test_f1}')\n",
    "\n",
    "# y_test_prediction = pd.DataFrame(test_output, columns=['prediction'])\n",
    "\n",
    "# # Save predictions to a CSV file\n",
    "# y_test_prediction.to_csv('Gradient_Boosting_Answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Classifier with data feature scaling and data imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_sm: 25.0 % of the dataset\n",
      "asd_sm: 25.0 % of the dataset\n",
      "kd_sm: 25.0 % of the dataset\n",
      "rhd_sm: 25.0 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "X = raw_data.drop('state', axis=1)\n",
    "y = raw_data['state']\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "def Standard_Scaler (df, col_names):\n",
    "    features = df[col_names]\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    df[col_names] = features\n",
    "    \n",
    "    return df\n",
    "\n",
    "col_names = X.columns\n",
    "\n",
    "X_train = Standard_Scaler(X_train, col_names)\n",
    "X_validation = Standard_Scaler(X_validation, col_names)\n",
    "\n",
    "resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "\n",
    "X_train_resampled, y_train_resampled = resample.fit_resample(X_train, y_train)\n",
    "\n",
    "print('normal_sm:', round(y_train_resampled.value_counts()[0]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('asd_sm:', round(y_train_resampled.value_counts()[1]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('kd_sm:', round(y_train_resampled.value_counts()[2]/len(y_train_resampled) * 100,2), '% of the dataset')\n",
    "print('rhd_sm:', round(y_train_resampled.value_counts()[3]/len(y_train_resampled) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Classifier with data feature scaling and data imbalance handling\n",
      "--------------------------------------------------\n",
      "Gradient Boosted Classifier: Training F1 Score: 0.9520082391905339 | Validation F1 Score: 0.16186286575895187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf_GBC.fit(X_train_resampled, y_train_resampled)\n",
    "training_output = clf_GBC.predict(X_train_resampled)\n",
    "training_f1 = f1_score(y_train_resampled, training_output, average='macro')\n",
    "\n",
    "Validation_Output = clf_GBC.predict(X_validation)\n",
    "Validation_f1 = f1_score(y_validation, Validation_Output, average='macro')\n",
    "\n",
    "print('Gradient Boosted Classifier with data feature scaling and data imbalance handling')\n",
    "print('-' * 50)\n",
    "print(f'Gradient Boosted Classifier: Training F1 Score: {training_f1} | Validation F1 Score: {Validation_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Classifier with data feature scaling, data imbalance handling, and hyperparameter tuning using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-10 22:35:58,636] A new study created in memory with name: no-name-0cb1ebd8-4ea6-4646-bfeb-92dba4e84dc3\n",
      "[I 2024-05-10 22:46:52,333] Trial 0 finished with value: 0.16123998028452957 and parameters: {'n_estimators': 172, 'learning_rate': 1.3771050461802645}. Best is trial 0 with value: 0.16123998028452957.\n",
      "[I 2024-05-10 22:49:13,230] Trial 1 finished with value: 0.223959791200548 and parameters: {'n_estimators': 39, 'learning_rate': 0.9797582810955014}. Best is trial 1 with value: 0.223959791200548.\n",
      "[I 2024-05-10 23:00:15,666] Trial 2 finished with value: 0.1714985407424226 and parameters: {'n_estimators': 175, 'learning_rate': 1.036957476835771}. Best is trial 1 with value: 0.223959791200548.\n",
      "[I 2024-05-10 23:03:18,840] Trial 3 finished with value: 0.21036931909628948 and parameters: {'n_estimators': 53, 'learning_rate': 1.4019199638421274}. Best is trial 1 with value: 0.223959791200548.\n",
      "[I 2024-05-10 23:10:23,922] Trial 4 finished with value: 0.17300532855136752 and parameters: {'n_estimators': 120, 'learning_rate': 0.8008454349148438}. Best is trial 1 with value: 0.223959791200548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.223959791200548\n",
      "  Params: \n",
      "    n_estimators: 39\n",
      "    learning_rate: 0.9797582810955014\n"
     ]
    }
   ],
   "source": [
    "def create_objective(X_train, y_train, X_val, y_val):\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 1, 200)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.5, 1.5)\n",
    "    \n",
    "        clf_GBC = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=1, random_state=0)\n",
    "        clf_GBC.fit(X_train, y_train)\n",
    "        \n",
    "        Validation_Output = clf_GBC.predict(X_val)\n",
    "        Validation_f1 = f1_score(y_val, Validation_Output, average='macro')\n",
    "        \n",
    "        return Validation_f1\n",
    "    \n",
    "    return objective\n",
    "\n",
    "# Now create the objective function using the closure\n",
    "objective = create_objective(X_train_resampled, y_train_resampled, X_validation, y_validation)\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
